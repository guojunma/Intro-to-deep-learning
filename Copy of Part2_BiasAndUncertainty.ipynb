{"cells":[{"cell_type":"markdown","metadata":{"id":"Kxl9-zNYhxlQ"},"source":["<table align=\"center\">\n","  <td align=\"center\"><a target=\"_blank\" href=\"http://introtodeeplearning.com\">\n","        <img src=\"https://i.ibb.co/Jr88sn2/mit.png\" style=\"padding-bottom:5px;\" />\n","      Visit MIT Deep Learning</a></td>\n","  <td align=\"center\"><a target=\"_blank\" href=\"https://colab.research.google.com/github/aamini/introtodeeplearning/blob/master/lab3/Part2_BiasAndUncertainty.ipynb\">\n","        <img src=\"https://i.ibb.co/2P3SLwK/colab.png\"  style=\"padding-bottom:5px;\" />Run in Google Colab</a></td>\n","  <td align=\"center\"><a target=\"_blank\" href=\"https://github.com/aamini/introtodeeplearning/blob/master/lab3/Part2_BiasAndUncertainty.ipynb\">\n","        <img src=\"https://i.ibb.co/xfJbPmL/github.png\"  height=\"70px\" style=\"padding-bottom:5px;\"  />View Source on GitHub</a></td>\n","</table>\n","\n","# Copyright Information"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"aAcJJN3Xh3S1","executionInfo":{"status":"ok","timestamp":1682965635839,"user_tz":420,"elapsed":2,"user":{"displayName":"Guojun Ma","userId":"15993449646633515284"}}},"outputs":[],"source":["# Copyright 2023 MIT Introduction to Deep Learning. All Rights Reserved.\n","# \n","# Licensed under the MIT License. You may not use this file except in compliance\n","# with the License. Use and/or modification of this code outside of MIT Introduction\n","# to Deep Learning must reference:\n","#\n","# © MIT Introduction to Deep Learning\n","# http://introtodeeplearning.com\n","#"]},{"cell_type":"markdown","metadata":{"id":"IgYKebt871EK"},"source":["# Laboratory 3: Debiasing, Uncertainty, and Robustness\n","\n","# Part 2: Mitigating Bias and Uncertainty in Facial Detection Systems\n","\n","In Lab 2, we defined a semi-supervised VAE (SS-VAE) to diagnose feature representation disparities and biases in facial detection systems. In Lab 3 Part 1, we gained experience with [Capsa](https://github.com/themis-ai/capsa/) and its ability to build risk-aware models automatically through wrapping. Now in this lab, we will put these two together: using Capsa to build systems that can *automatically* uncover and mitigate bias and uncertainty in facial detection systems.\n","\n","As we have seen, automatically detecting and mitigating bias and uncertainty is crucial to deploying fair and safe models. Building off our foundation with Capsa, developed by [Themis AI](https://themisai.io/), we will now use Capsa for the facial detection problem, in order to diagnose risks in facial detection models. You will then design and create strategies to mitigate these risks, with goal of improving model performance across the entire facial detection dataset.\n","\n","**Your goal in this lab -- and the associated competition -- is to design a strategic solution for bias and uncertainty mitigation, using Capsa.** The approaches and solutions with oustanding performance will be recognized with outstanding prizes! Details on the submission process are at the end of this lab.\n","\n","![Capsa overview](https://raw.githubusercontent.com/aamini/introtodeeplearning/2023/lab3/img/capsa_overview.png)"]},{"cell_type":"markdown","metadata":{"id":"6JTRoM7E71EU"},"source":["Let's get started by installing the necessary dependencies:"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"2PdAhs1371EU","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1682965670460,"user_tz":420,"elapsed":23896,"user":{"displayName":"Guojun Ma","userId":"15993449646633515284"}},"outputId":"da8929b3-6e85-435d-f560-c56553863e9e"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting git+https://github.com/aamini/introtodeeplearning.git@2023\n","  Cloning https://github.com/aamini/introtodeeplearning.git (to revision 2023) to /tmp/pip-req-build-ij_pc0pf\n","  Running command git clone --filter=blob:none --quiet https://github.com/aamini/introtodeeplearning.git /tmp/pip-req-build-ij_pc0pf\n","  Running command git checkout -b 2023 --track origin/2023\n","  Switched to a new branch '2023'\n","  Branch '2023' set up to track remote branch '2023' from 'origin'.\n","  Resolved https://github.com/aamini/introtodeeplearning.git to commit ed7501323f5b3f0f865db898b1ce6a0528df1586\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from mitdeeplearning==0.4.0) (1.22.4)\n","Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from mitdeeplearning==0.4.0) (2022.10.31)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from mitdeeplearning==0.4.0) (4.65.0)\n","Requirement already satisfied: gym in /usr/local/lib/python3.10/dist-packages (from mitdeeplearning==0.4.0) (0.25.2)\n","Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gym->mitdeeplearning==0.4.0) (2.2.1)\n","Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from gym->mitdeeplearning==0.4.0) (0.0.8)\n","Building wheels for collected packages: mitdeeplearning\n","  Building wheel for mitdeeplearning (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for mitdeeplearning: filename=mitdeeplearning-0.4.0-py3-none-any.whl size=2118523 sha256=31b3a71e41e6cdd14d78d2cd3c032cba76d1fe03a14f989987afdd363ded5ca4\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-uh_97npv/wheels/ef/52/04/ba06741bd3ca87d9d5a73710a375adad24618e18924cc54e32\n","Successfully built mitdeeplearning\n","Installing collected packages: mitdeeplearning\n","Successfully installed mitdeeplearning-0.4.0\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting capsa\n","  Downloading capsa-0.1.5.tar.gz (48 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.7/48.7 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Building wheels for collected packages: capsa\n","  Building wheel for capsa (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for capsa: filename=capsa-0.1.5-py3-none-any.whl size=49319 sha256=28b1c9ea5a7461657edd89e5a1e3d60b9a1c011f24eea6c4d7d987fbaf80db07\n","  Stored in directory: /root/.cache/pip/wheels/71/64/67/00ffc339717c0fd69e2bfd430a5d097eab9841c0bd32b6a95e\n","Successfully built capsa\n","Installing collected packages: capsa\n","Successfully installed capsa-0.1.5\n"]}],"source":["# Import Tensorflow 2.0\n","#%tensorflow_version 2.x\n","import tensorflow as tf\n","\n","import IPython\n","import functools\n","import matplotlib.pyplot as plt\n","import numpy as np\n","from tqdm import tqdm\n","\n","# Download and import the MIT 6.S191 package\n","!pip install git+https://github.com/aamini/introtodeeplearning.git@2023\n","import mitdeeplearning as mdl\n","\n","# Download and import capsa\n","!pip install capsa\n","import capsa"]},{"cell_type":"markdown","metadata":{"id":"6VKVqLb371EV"},"source":["# 3.1 Datasets\n","\n","Since we are again focusing on the facial detection problem, we will use the same datasets from Lab 2. To remind you, we have a dataset of positive examples (i.e., of faces) and a dataset of negative examples (i.e., of things that are not faces).\n","\n","1.   **Positive training data**: [CelebA Dataset](http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html). A large-scale dataset (over 200K images) of celebrity faces.   \n","2.   **Negative training data**: [ImageNet](http://www.image-net.org/). A large-scale dataset with many images across many different categories. We will take negative examples from a variety of non-human categories.\n","\n","We will evaluate trained models on an independent test dataset of face images to diagnose and mitigate potential issues with *bias, fairness, and confidence*. This will be a larger test dataset for evaluation purposes.\n","\n","We begin by importing these datasets. We have defined a `DatasetLoader` class that does a bit of data pre-processing to import the training data in a usable format."]},{"cell_type":"code","execution_count":4,"metadata":{"id":"HIA6EA1D71EW","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1682965752077,"user_tz":420,"elapsed":65010,"user":{"displayName":"Guojun Ma","userId":"15993449646633515284"}},"outputId":"fd47fd09-7e2b-4eba-b94f-49d4706b06f5"},"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading data from https://www.dropbox.com/s/tbra3danrk5x8h5/train_face_2023_perturbed_small.h5?dl=1\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n","  and should_run_async(code)\n"]},{"output_type":"stream","name":"stdout","text":["675368621/675368621 [==============================] - 61s 0us/step\n","Opening /root/.keras/datasets/train_face_2023_perturbed_small.h5\n","Loading data into memory...\n","Opening /root/.keras/datasets/train_face_2023_perturbed_small.h5\n","Loading data into memory...\n"]}],"source":["batch_size = 32\n","\n","# Get the training data: both images from CelebA and ImageNet\n","path_to_training_data = tf.keras.utils.get_file('train_face_2023_perturbed_small.h5', 'https://www.dropbox.com/s/tbra3danrk5x8h5/train_face_2023_perturbed_small.h5?dl=1')\n","# Instantiate a DatasetLoader using the downloaded dataset\n","train_loader = mdl.lab3.DatasetLoader(path_to_training_data, training=True, batch_size=batch_size)\n","test_loader = mdl.lab3.DatasetLoader(path_to_training_data, training=False, batch_size=batch_size)"]},{"cell_type":"markdown","metadata":{"id":"cREmhMWJ71EX"},"source":["### Building robustness to bias and uncertainty\n","\n","Remember that we'll be training our facial detection classifiers on the large, well-curated CelebA dataset (and ImageNet), and then evaluating their accuracy by testing them on an independent test dataset. We want to mitigate the effects of unwanted bias and uncertainty on the model's predictions and performance. Your goal is to build the best-performing, most robust model, one that achieves high classification accuracy across the entire test dataset.\n","\n","To achieve this, you may want to consider the three metrics introduced with Capsa: (1) representation bias, (2) data or aleatoric uncertainty, and (3) model or epistemic uncertainty. Note that all three of these metrics are different! For example, we can have well-represented examples that still have high epistemic uncertainty. Think about how you may use these metrics to improve the performance of your model."]},{"cell_type":"markdown","metadata":{"id":"1NhotGiT71EY"},"source":["# 3.2 Risk-aware facial detection with Capsa\n","\n","In Lab 2, we built a semi-supervised variational autoencoder (SS-VAE) to learn the latent structure of our database and to uncover feature representation disparities, inspired by the approach of [uncover hidden biases](http://introtodeeplearning.com/AAAI_MitigatingAlgorithmicBias.pdf). In this lab, we'll show that we can use Capsa to build the same VAE in one line!\n","\n","This sets the foundation for quantifying a key risk metric -- representation bias -- for the facial detection problem. In working to improve your model's performance, you will want to consider representation bias carefully and think about how you could mitigate the effect of representation bias.\n","\n","Just like in Lab 2, we begin by defining a standard CNN-based classifier. We will then use Capsa to wrap the model and build the risk-aware VAE variant."]},{"cell_type":"code","execution_count":5,"metadata":{"id":"5hQb75Vm71EZ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1682965760426,"user_tz":420,"elapsed":2,"user":{"displayName":"Guojun Ma","userId":"15993449646633515284"}},"outputId":"9edf5dfd-2a78-4ee8-ce82-1fb93665a207"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n","  and should_run_async(code)\n"]}],"source":["### Define the CNN classifier model ###\n","\n","'''Function to define a standard CNN model'''\n","def make_standard_classifier(n_outputs=1, n_filters=12):\n","  Conv2D = functools.partial(tf.keras.layers.Conv2D, padding='same', activation='relu')\n","  BatchNormalization = tf.keras.layers.BatchNormalization\n","  Flatten = tf.keras.layers.Flatten\n","  Dense = functools.partial(tf.keras.layers.Dense, activation='relu')\n","\n","  model = tf.keras.Sequential([ \n","    tf.keras.Input(shape=(64,64, 3)),\n","    Conv2D(filters=1*n_filters, kernel_size=5,  strides=2),\n","    BatchNormalization(),\n","    \n","    Conv2D(filters=2*n_filters, kernel_size=5,  strides=2),\n","    BatchNormalization(),\n","\n","    Conv2D(filters=4*n_filters, kernel_size=3,  strides=2),\n","    BatchNormalization(),\n","\n","    Conv2D(filters=6*n_filters, kernel_size=3,  strides=2),\n","    BatchNormalization(),\n","\n","    Conv2D(filters=8*n_filters, kernel_size=3,  strides=2),\n","    BatchNormalization(),\n","\n","    Flatten(),\n","    Dense(512),\n","    Dense(n_outputs, activation=None),\n","  ])\n","  return model"]},{"cell_type":"markdown","metadata":{"id":"LgTG6buf71Ea"},"source":["### Capsa's `HistogramVAEWrapper`\n","\n","With our base classifier Capsa allows us to automatically define a VAE implementing that base classifier. Capsa's [`HistogramVAEWrapper`](https://themisai.io/capsa/api_documentation/HistogramVAEWrapper.html) builds this VAE to analyze the latent space distribution, just as we did in Lab 2. \n","\n","Specifically, `capsa.HistogramVAEWrapper` constructs a histogram with `num_bins` bins across every dimension of the latent space, and then calculates the joint probability of every sample according to the constructed histograms. The samples with the lowest joint probability have the lowest representation; the samples with the highest joint probability have the highest representation.\n","\n","`capsa.HistogramVAEWrapper` takes in a number of arguments including:\n","1. `base_model`: the model to be transformed into the risk-aware variant.\n","2. `num_bins`: the number of bins we want to discretize our distribution into. \n","2. `queue_size`: the number of samples we want to track at any given point.\n","3. `decoder`: the decoder architecture for the VAE.\n","\n","We define the same decoder as in Lab 2:"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"zTat3K8E71Eb","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1682965763918,"user_tz":420,"elapsed":1,"user":{"displayName":"Guojun Ma","userId":"15993449646633515284"}},"outputId":"b9dfe668-3eb4-4ef0-e5a4-fd0ee7b82cfb"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n","  and should_run_async(code)\n"]}],"source":["### Define the decoder architecture for the facial detection VAE ###\n","\n","def make_face_decoder_network(n_filters=12):\n","  # Functionally define the different layer types we will use\n","  Conv2DTranspose = functools.partial(tf.keras.layers.Conv2DTranspose, \n","                                      padding='same', activation='relu')\n","  BatchNormalization = tf.keras.layers.BatchNormalization\n","  Flatten = tf.keras.layers.Flatten\n","  Dense = functools.partial(tf.keras.layers.Dense, activation='relu')\n","  Reshape = tf.keras.layers.Reshape\n","\n","  # Build the decoder network using the Sequential API\n","  decoder = tf.keras.Sequential([\n","    # Transform to pre-convolutional generation\n","    Dense(units=2*2*8*n_filters),  # 4x4 feature maps (with 6N occurances)\n","    Reshape(target_shape=(2, 2, 8*n_filters)),\n","\n","    # Upscaling convolutions (inverse of encoder)\n","    Conv2DTranspose(filters=6*n_filters, kernel_size=3,  strides=2),\n","    Conv2DTranspose(filters=4*n_filters, kernel_size=3,  strides=2),\n","    Conv2DTranspose(filters=2*n_filters, kernel_size=3,  strides=2),\n","    Conv2DTranspose(filters=1*n_filters, kernel_size=5,  strides=2),\n","    Conv2DTranspose(filters=3, kernel_size=5,  strides=2),\n","  ])\n","\n","  return decoder"]},{"cell_type":"markdown","metadata":{"id":"SzFGcrhv71Ed"},"source":["We are ready to create the wrapped model using `capsa.HistogramVAEWrapper` by passing in the relevant arguments!\n","\n","Just like in the wrappers in the Introduction to Capsa lab, we can take our standard CNN classifier, wrap it with `capsa.HistogramVAEWrapper`, build the wrapped model. The wrapper then enablings semi-supervised training for the facial detection task. As the wrapped model trains, the classifier weights are updated, and the VAE-wrapped model learns to track feature distributions over the latent space. More details of the `HistogramVAEWrapper` and how it can be used are [available here](https://themisai.io/capsa/api_documentation/HistogramVAEWrapper.html).\n","\n","We can then evaluate the representation bias of the classifier on the test dataset. By calling the `wrapped_model` on our test data, we can automatically generate representation bias and uncertainty scores that are normally manually calculated. Let's wrap our base CNN classifier using Capsa, train and build the resulting model, and start to process the test data: "]},{"cell_type":"code","execution_count":9,"metadata":{"id":"YqsBHBf3yUlm","colab":{"base_uri":"https://localhost:8080/","height":283},"executionInfo":{"status":"error","timestamp":1682966669154,"user_tz":420,"elapsed":8187,"user":{"displayName":"Guojun Ma","userId":"15993449646633515284"}},"outputId":"513351ee-a69e-4454-df1b-50e9cdfe754b"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n","  and should_run_async(code)\n"]},{"output_type":"stream","name":"stdout","text":["9/9 [==============================] - 6s 697ms/step\n"]},{"output_type":"error","ename":"ValueError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-9-15afe91930f5>\u001b[0m in \u001b[0;36m<cell line: 25>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;31m# Call the Capsa-wrapped classifier to generate outputs: predictions, uncertainty, and bias!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muncertainty\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwrapped_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_imgs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m512\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 3)"]}],"source":["### Estimating representation bias with Capsa HistogramVAEWrapper ###\n","\n","model = make_standard_classifier()\n","# Wrap the CNN classifier for latent encoding with a VAE wrapper\n","wrapped_model = capsa.HistogramVAEWrapper(model, num_bins=5, queue_size=20000, \n","    latent_dim = 32, decoder=make_face_decoder_network())\n","\n","# Build the model for classification, defining the loss function, optimizer, and metrics\n","wrapped_model.compile(\n","    optimizer=tf.keras.optimizers.Adam(learning_rate=5e-4),\n","    loss=tf.keras.losses.BinaryCrossentropy(from_logits=True), # for classification\n","    metrics=[tf.keras.metrics.BinaryAccuracy()], # for classification\n","    run_eagerly=True\n",")\n","\n","# Train the wrapped model for 6 epochs by fitting to the training data\n","history = wrapped_model.fit(\n","        train_loader,\n","        epochs=6,\n","        batch_size=batch_size,\n",")\n","\n","## Evaluation\n","\n","# Get all faces from the testing dataset\n","test_imgs = test_loader.get_all_faces()\n","\n","# Call the Capsa-wrapped classifier to generate outputs: predictions, uncertainty, and bias!\n","predictions, uncertainty, bias = wrapped_model.predict(test_imgs, batch_size=512)"]},{"cell_type":"markdown","metadata":{"id":"629ng-_H6WOk"},"source":["# 3.3 Analyzing representation bias with Capsa\n","\n","From the above output, we have an estimate for the representation bias score! We can analyze the representation scores to start to think about manifestations of bias in the facial detection dataset. Before you run the next code block, which faces would you expect to be underrepresented in the dataset? Which ones do you think will be overrepresented?"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"OYMRqq5E71Ee","colab":{"base_uri":"https://localhost:8080/","height":301},"executionInfo":{"status":"error","timestamp":1682966635738,"user_tz":420,"elapsed":3,"user":{"displayName":"Guojun Ma","userId":"15993449646633515284"}},"outputId":"d179e529-5271-445f-c6bd-40012ec39efe"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n","  and should_run_async(code)\n"]},{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-8-1be8b9c3ab2f>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Sort according to lowest to highest representation scores\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margsort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# sort the score values themselves\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0msorted_images\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_imgs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# sort images from lowest to highest representations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0msorted_biases\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# order the representation bias scores\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'bias' is not defined"]}],"source":["### Analyzing representation bias scores ###\n","\n","# Sort according to lowest to highest representation scores\n","indices = np.argsort(bias, axis=None) # sort the score values themselves\n","sorted_images = test_imgs[indices] # sort images from lowest to highest representations\n","sorted_biases = bias[indices] # order the representation bias scores\n","sorted_preds = predictions[indices] # order the prediction values\n","\n","\n","# Visualize the 20 images with the lowest and highest representation in the test dataset\n","fig, ax = plt.subplots(1, 2, figsize=(16, 8))\n","ax[0].imshow(mdl.util.create_grid_of_images(sorted_images[-20:], (4, 5)))\n","ax[0].set_title(\"Over-represented\")\n","\n","ax[1].imshow(mdl.util.create_grid_of_images(sorted_images[:20], (4, 5)))\n","ax[1].set_title(\"Under-represented\");"]},{"cell_type":"markdown","metadata":{"id":"-JYmGMJF71Ef"},"source":["We can also quantify how the representation density relates to the classification accuracy by plotting the two against each other:"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"DfzlOhWi71Ef","colab":{"base_uri":"https://localhost:8080/","height":702},"executionInfo":{"status":"error","timestamp":1682966705789,"user_tz":420,"elapsed":432,"user":{"displayName":"Guojun Ma","userId":"15993449646633515284"}},"outputId":"a9cc3fa8-4568-41e5-f120-0be9f44955ce"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n","  and should_run_async(code)\n"]},{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-10-c9fd39268806>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Density (Representation)\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Accuracy\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0maveraged_imgs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmdl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlab3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot_accuracy_vs_risk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msorted_images\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msorted_biases\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msorted_preds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Bias vs. Accuracy\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'sorted_images' is not defined"]},{"output_type":"display_data","data":{"text/plain":["<Figure size 640x480 with 1 Axes>"],"image/png":"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\n"},"metadata":{}}],"source":["# Plot the representation density vs. the accuracy\n","plt.xlabel(\"Density (Representation)\")\n","plt.ylabel(\"Accuracy\")\n","averaged_imgs = mdl.lab3.plot_accuracy_vs_risk(sorted_images, sorted_biases, sorted_preds, \"Bias vs. Accuracy\")"]},{"cell_type":"markdown","metadata":{"id":"i8ERzg2-71Ef"},"source":["These representations scores relate back to data examples, so we can visualize what the average face looks like for a given *percentile* of representation density:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kn9IpPKYSECg"},"outputs":[],"source":["fig, ax = plt.subplots(figsize=(15,5))\n","ax.imshow(mdl.util.create_grid_of_images(averaged_imgs, (1,10)))"]},{"cell_type":"markdown","metadata":{"id":"cRNV-3SU71Eg"},"source":["#### **TODO: Scoring representation densities with Capsa**\n","\n","Write short answers to the questions below to complete the `TODO`s:\n","\n","1. How does accuracy relate to the representation score? From this relationship, what can you determine about the bias underlying the dataset?\n","2. What does the average face in the 10th percentile of representation density look like (i.e., the face for which 10% of the data have lower probability of occuring)? What about the 90th percentile? What changes across these faces?\n","3. What could be potential limitations of the `HistogramVAEWrapper` approach as it is implemented now?"]},{"cell_type":"markdown","metadata":{"id":"ww5lx7ue71Eg"},"source":["# 3.4 Analyzing epistemic uncertainty with Capsa\n","\n","Recall that *epistemic* uncertainty, or a model's uncertainty in its prediction, can arise from out-of-distribution data, missing data, or samples that are harder to learn. This does not necessarily correlate with representation bias! Imagine the scenario of training an object detector for self-driving cars: even if the model is presented with many cluttered scenes, these samples still may be harder to learn than scenes with very few objects in them.\n","\n","We will now use our VAE-wrapped facial detection classifier to analyze and estimate the epistemic uncertainty of the model trained on the facial detection task.\n","\n","While most methods of estimating epistemic uncertainty are *sampling-based*, we can also use ***reconstruction-based*** methods -- like using VAEs -- to estimate epistemic uncertainty. If a model is unable to provide a good reconstruction for a given data point, it has not learned that area of the underlying data distribution well, and therefore has high epistemic uncertainty.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"NEfeWo2p7wKm"},"source":["Since we've already used the `HistogramVAEWrapper` to calculate the histograms for representation bias quantification, we can use the exact same VAE wrapper to shed insight into epistemic uncertainty! Capsa helps us do exactly that. When we called the model, we returned the classification prediction, uncertainty, and bias for every sample:\n","`predictions, uncertainty, bias = wrapped_model.predict(test_imgs, batch_size=512)`.\n","\n","Let's analyze these estimated uncertainties:"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"AwGPvdZm71Eg","colab":{"base_uri":"https://localhost:8080/","height":301},"executionInfo":{"status":"error","timestamp":1682966804000,"user_tz":420,"elapsed":3,"user":{"displayName":"Guojun Ma","userId":"15993449646633515284"}},"outputId":"7485717f-58ce-4b88-aee7-5f511a53de9d"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n","  and should_run_async(code)\n"]},{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-11-1e798e298809>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Sort according to epistemic uncertainty estimates\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mepistemic_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margsort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muncertainty\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# sort the uncertainty values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mepistemic_images\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_imgs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mepistemic_indices\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# sort images from lowest to highest uncertainty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0msorted_epistemic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0muncertainty\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mepistemic_indices\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# order the uncertainty scores\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'uncertainty' is not defined"]}],"source":["### Analyzing epistemic uncertainty estimates ###\n","\n","# Sort according to epistemic uncertainty estimates\n","epistemic_indices = np.argsort(uncertainty, axis=None) # sort the uncertainty values\n","epistemic_images = test_imgs[epistemic_indices] # sort images from lowest to highest uncertainty\n","sorted_epistemic = uncertainty[epistemic_indices] # order the uncertainty scores\n","sorted_epistemic_preds = predictions[epistemic_indices] # order the prediction values\n","\n","\n","# Visualize the 20 images with the LEAST and MOST epistemic uncertainty\n","fig, ax = plt.subplots(1, 2, figsize=(16, 8))\n","ax[0].imshow(mdl.util.create_grid_of_images(epistemic_images[:20], (4, 5)))\n","ax[0].set_title(\"Least Uncertain\");\n","\n","ax[1].imshow(mdl.util.create_grid_of_images(epistemic_images[-20:], (4, 5)))\n","ax[1].set_title(\"Most Uncertain\");"]},{"cell_type":"markdown","metadata":{"id":"L0dA8EyX71Eh"},"source":["We quantify how the epistemic uncertainty relates to the classification accuracy by plotting the two against each other:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rzQwvSvA71Eh"},"outputs":[],"source":["# Plot epistemic uncertainty vs. classification accuracy\n","plt.xlabel(\"Epistemic Uncertainty\")\n","plt.ylabel(\"Accuracy\")\n","_ = mdl.lab3.plot_accuracy_vs_risk(epistemic_images, sorted_epistemic, sorted_epistemic_preds, \"Epistemic Uncertainty vs. Accuracy\")"]},{"cell_type":"markdown","metadata":{"id":"iyn0IE6x71Eh"},"source":["#### **TODO: Estimating epistemic uncertainties with Capsa**\n","\n","Write short answers to the questions below to complete the `TODO`s:\n","\n","1. How does accuracy relate to the epistemic uncertainty?\n","2. How do the results for epistemic uncertainty compare to the results for representation bias? Was this expected or unexpted? Why?\n","3. What may be instances in the facial detection task that could have high representation density but also high uncertainty? "]},{"cell_type":"markdown","metadata":{"id":"XbwRbesM71Eh"},"source":["# 3.4 Resampling based on risk metrics\n","\n","Finally, we will use the risk metrics just computed to actually *mitigate* the issues of bias and uncertainty in the facial detection classifier.\n","\n","Specifically, we will use the latent variables learned via the VAE to adaptively re-sample the face (CelebA) data during training, following the approach of [recent work](http://introtodeeplearning.com/AAAI_MitigatingAlgorithmicBias.pdf). We will alter the probability that a given image is used during training based on how often its latent features appear in the dataset. So, faces with rarer features (like dark skin, sunglasses, or hats) should become more likely to be sampled during training, while the sampling probability for faces with features that are over-represented in the training dataset should decrease (relative to uniform random sampling across the training data).\n","\n","Note that we want to debias and amplify only the *positive* samples in the dataset -- the faces -- so we are going to only adjust probabilities and calculate scores for these samples. We focus on using the representation bias scores to implement this adaptive resampling to achieve model debiasing.\n","\n","We re-define the wrapped model with `HistogramVAEWrapper`, and then define the adaptive resampling operation for training. At each training epoch, we compute the predictions, uncertainties, and representation bias scores, then recompute the data sampling probabilities according to the *inverse* of the representation bias score. That is, samples with higher representation densities will end up with lower re-sampling probabilities; samples with lower representations will end up with higher re-sampling probabilities.\n","\n","Let's do all this below!"]},{"cell_type":"code","execution_count":12,"metadata":{"id":"Q99jG0dB71Ei","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1682966818139,"user_tz":420,"elapsed":3823,"user":{"displayName":"Guojun Ma","userId":"15993449646633515284"}},"outputId":"493189f7-9d9c-45e7-8c32-baf1dcf12a5e"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n","  and should_run_async(code)\n"]}],"source":["### Define the standard CNN classifier and wrap with HistogramVAE ###\n","\n","classifier = make_standard_classifier()\n","# Wrap with HistogramVAE\n","wrapper = capsa.HistogramVAEWrapper(classifier, latent_dim=32, num_bins=5, \n","                          queue_size=2000, decoder=make_face_decoder_network())\n","\n","# Build the wrapped model for the classification task\n","wrapper.compile(optimizer=tf.keras.optimizers.Adam(5e-4),\n","                loss=tf.keras.losses.BinaryCrossentropy(),\n","                metrics=[tf.keras.metrics.BinaryAccuracy()])\n","\n","# Load training data\n","train_imgs = train_loader.get_all_faces()"]},{"cell_type":"code","execution_count":13,"metadata":{"id":"odFu4u7i71Ei","colab":{"base_uri":"https://localhost:8080/","height":471},"executionInfo":{"status":"error","timestamp":1682966821402,"user_tz":420,"elapsed":4,"user":{"displayName":"Guojun Ma","userId":"15993449646633515284"}},"outputId":"afdc82ef-6311-4907-b006-35d855515904"},"outputs":[{"output_type":"stream","name":"stdout","text":["Starting epoch 1/6\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n","  and should_run_async(code)\n"]},{"output_type":"error","ename":"AttributeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-13-70b95627bf44>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m   \u001b[0;31m# Get a batch of training data and compute the training step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m100\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/capsa/bias/histogramvae.py\u001b[0m in \u001b[0;36mtrain_step\u001b[0;34m(self, data, prefix)\u001b[0m\n\u001b[1;32m    225\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGradientTape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m             \u001b[0mmetric_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_hat\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbias\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 227\u001b[0;31m             compiled_loss = self.compiled_loss(\n\u001b[0m\u001b[1;32m    228\u001b[0m                 \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_hat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mregularization_losses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlosses\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m             )\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/engine/compile_utils.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, y_true, y_pred, sample_weight, regularization_losses)\u001b[0m\n\u001b[1;32m    261\u001b[0m                 \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 263\u001b[0;31m             \u001b[0my_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_p\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmatch_dtype_and_rank\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_p\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    264\u001b[0m             \u001b[0msw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlosses_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_p\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlosses_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_p\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m             \u001b[0mloss_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_obj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_p\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/engine/compile_utils.py\u001b[0m in \u001b[0;36mmatch_dtype_and_rank\u001b[0;34m(y_t, y_p, sw)\u001b[0m\n\u001b[1;32m    829\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmatch_dtype_and_rank\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_p\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m     \u001b[0;34m\"\"\"Match dtype and rank of predictions.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 831\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0my_t\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrank\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0my_p\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrank\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    832\u001b[0m         \u001b[0my_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    833\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msw\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'rank'"]}],"source":["### Debiasing via resampling based on risk metrics ###\n","\n","# The training loop -- outer loop iterates over the number of epochs\n","num_epochs = 6\n","for i in range(num_epochs):\n","  print(\"Starting epoch {}/{}\".format(i+1, num_epochs))\n","  \n","  # Get a batch of training data and compute the training step\n","  for step, data in enumerate(train_loader):\n","    metrics = wrapper.train_step(data)\n","    if step % 100 == 0:\n","        print(step)\n","\n","  # After the epoch is done, recompute data sampling proabilities \n","  #  according to the inverse of the bias\n","  pred, unc, bias = wrapper(train_imgs)\n","\n","  # Increase the probability of sampling under-represented datapoints by setting \n","  #   the probability to the **inverse** of the biases\n","  inverse_bias = 1.0 / (bias.numpy() + 1e-7)\n","\n","  # Normalize the inverse biases in order to convert them to probabilities\n","  p_faces = inverse_bias / np.sum(inverse_bias)\n","\n","  # Update the training data loader to sample according to this new distribution\n","  train_loader.p_pos = p_faces"]},{"cell_type":"markdown","metadata":{"id":"SwXrAeBo71Ej"},"source":["That's it! We should have a debiased model (we hope!). Let's see how the model does.\n","\n","### Evaluation\n","\n","Let's run the same analyses as before, and plot the classification accuracy vs. the representation bias and classification accuracy vs. epistemic uncertainty. We want the model to do better across the data samples, achieving higher accuracies on the under-represented and more uncertain samples compared to previously.\n"]},{"cell_type":"code","execution_count":14,"metadata":{"id":"yaMnwd_w71Ej","colab":{"base_uri":"https://localhost:8080/","height":319},"executionInfo":{"status":"error","timestamp":1682966871284,"user_tz":420,"elapsed":41868,"user":{"displayName":"Guojun Ma","userId":"15993449646633515284"}},"outputId":"10d01118-0cd3-4413-de69-ba13acb6cb50"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n","  and should_run_async(code)\n"]},{"output_type":"stream","name":"stdout","text":["141/141 [==============================] - 23s 157ms/step\n"]},{"output_type":"error","ename":"ValueError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-14-108f52443aa6>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Get classification predictions, uncertainties, and representation bias scores\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_imgs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Sort according to lowest to highest representation scores\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 3)"]}],"source":["### Evaluation of debiased model ###\n","\n","# Get classification predictions, uncertainties, and representation bias scores\n","pred, unc, bias = wrapper.predict(test_imgs)\n","\n","# Sort according to lowest to highest representation scores\n","indices = np.argsort(bias, axis=None)\n","bias_images = test_imgs[indices] # sort the images\n","sorted_bias = bias[indices] # sort the representation bias scores\n","sorted_bias_preds = pred[indices] # sort the predictions\n","\n","# Plot the representation bias vs. the accuracy\n","plt.xlabel(\"Density (Representation)\")\n","plt.ylabel(\"Accuracy\")\n","_ = mdl.lab3.plot_accuracy_vs_risk(bias_images, sorted_bias, sorted_bias_preds, \"Bias vs. Accuracy\")"]},{"cell_type":"markdown","metadata":{"id":"d1cEEnII71Ej"},"source":["# 3.5 Competition!\n","\n","Now, you are well equipped to submit to the competition to dig in deeper into deep learning models, uncover their deficiencies with Capsa, address those deficiencies, and submit your findings!\n","\n","**Below are some potential areas to start investigating -- the goal of the competition is to develop creative and innovative solutions to address bias and uncertainty, and to improve the overall performance of deep learning models.**\n","\n","\n","We encourage you to identify other questions that could be solved with Capsa and use those as the basis of your submission. But, to help get you started, here are some interesting questions that you might look into solving with these new tools and knowledge that you've built up: \n","\n","1. In this lab, you learned how to build a wrapper that can estimate the bias within the training data, and take the results from this wrapper to adaptively re-sample during training to encourage learning on under-represented data. \n","  * Can we apply a similar approach to mitigate epistemic uncertainty in the model? \n","  * Can this approach be combined with your original bias mitigation approach to achieve robustness across both bias *and* uncertainty? \n","\n","2. In this lab, you focused on the `HistogramVAEWrapper`. \n","  * How can you use other methods of uncertainty in Capsa to strengthen your uncertainty estimates? Checkout [Capsa documentation](https://themisai.io/capsa/api_documentation/index.html) for a list of all wrappers, and ask for help if you run into trouble applying them to your model!\n","  * Can you combine uncertainty estimates from different wrappers to achieve greater robustness in your estimates? \n","\n","3. So far in this part of the lab, we focused only on bias and epistemic uncertainty. What about aleatoric uncetainty? \n","  * We've curated a dataset (available at [this URL](https://www.dropbox.com/s/wsdyma8a340k8lw/train_face_2023_perturbed_large.h5?dl=0)) of faces with greater amounts of aleatoric uncertainty -- can you use Capsa to wrap your model, estimate aleatoric uncertainty, and remove it from the dataset? \n","  * Does removing aleatoric uncertainty help improve your training accuracy on this new dataset? \n","  * Can you develop an approach to incorporate this aleatoric uncertainty estimation into the predictive training pipeline in order to improve accuracy? You may find some surprising results!!\n","\n","4. How can the performance of the classifier above be improved even further? We purposely did not optimize hyperparameters to leave this up to you!\n","\n","5. Are there other applications that you think Capsa and bias/uncertainty estimation would be helpful in? \n","  * Try integrating Capsa into another domain or dataset and submit your findings!\n","  * Are there applications where you may *not* want to debias your model? \n","\n","\n","**To enter the competition, please upload the following to the [lab submission site](https://www.dropbox.com/request/TTYz3Ikx5wIgOITmm5i2):**\n","\n","* Written short-answer responses to `TODO`s from Lab 2, Part 2 on Facial Detection.\n","* Description of the wrappers, algorithms, and approach you used. What was your strategy? What wrappers did you implement? What debiasing or mitigation strategies did you try? How and why did these modifications affect performance? Describe *any* modifications or implementations you made to the template code, and what their effects were. Written text, visual diagram, and plots welcome!\n","* Jupyter notebook with the code you used to generate your results (along with all plots/visuals generated).\n","\n","**Name your file in the following format: `[FirstName]_[LastName]_Face`, followed by the file format (.zip, .ipynb, .pdf, etc).** ZIP files are preferred over individual files. If you submit individual files, you must name the individual files according to the above nomenclature (e.g., `[FirstName]_[LastName]_Face_TODO.pdf`, `[FirstName]_[LastName]_Face_Report.pdf`, etc.). **Submit your files [here](https://www.dropbox.com/request/TTYz3Ikx5wIgOITmm5i2).**\n","\n","We encourage you to think about and maybe even address some questions raised by this lab and dig into any questions that you may have about the risks inherrent to neural networks and their data. \n","\n","<img src=\"https://i.ibb.co/BjLSRMM/ezgif-2-253dfd3f9097.gif\" />"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[{"file_id":"https://github.com/aamini/introtodeeplearning/blob/master/lab3/Part2_BiasAndUncertainty.ipynb","timestamp":1681839682660}]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.6 (default, Oct 18 2022, 12:41:40) \n[Clang 14.0.0 (clang-1400.0.29.202)]"},"vscode":{"interpreter":{"hash":"31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"}}},"nbformat":4,"nbformat_minor":0}